{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fa088f",
   "metadata": {},
   "source": [
    "# üõ∞Ô∏è End-to-End Deep Learning Pipeline for Road Network Extraction from Satellite Imagery\n",
    "\n",
    "This project implements a complete, state-of-the-art deep learning pipeline for **semantic segmentation of road networks** from high-resolution satellite imagery. Using the **SpaceNet Roads Challenge dataset**, this work goes beyond a simple model implementation to tackle the complex, real-world challenges of **geospatial data processing, model optimization, and post-processing** to generate clean, connected road graphs.\n",
    "\n",
    "‚úÖ The final model achieves a validation **IoU of ~0.60**, demonstrating strong performance in identifying road pixels.  \n",
    "üõ†Ô∏è More importantly, the project includes a **post-processing pipeline** to convert raw pixel-level predictions into a **coherent road network**, suitable for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc55f3",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509034a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import EarlyStopping ,ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.warp import calculate_default_transform\n",
    "import geopandas as gpd\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "import cv2\n",
    "import sknw\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.morphology import skeletonize, remove_small_objects\n",
    "from scipy.ndimage import label\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee021d",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd144cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_data(output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the SpaceNet 3 dataset for Paris (Train and Test).\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): The directory to download and extract the data into.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Data will be downloaded and extracted to: '{output_dir}'\")\n",
    "\n",
    "    # List of files to download\n",
    "    s3_files_to_download = [\n",
    "        \"s3://spacenet-dataset/spacenet/SN3_roads/tarballs/SN3_roads_train_AOI_3_Paris.tar.gz\",\n",
    "        \"s3://spacenet-dataset/spacenet/SN3_roads/tarballs/SN3_roads_train_AOI_3_Paris_geojson_roads_speed.tar.gz\",\n",
    "        \"s3://spacenet-dataset/spacenet/SN3_roads/tarballs/SN3_roads_test_public_AOI_3_Paris.tar.gz\"\n",
    "    ]\n",
    "\n",
    "    tar_files_to_extract = [\n",
    "        \"SN3_roads_train_AOI_3_Paris.tar.gz\",\n",
    "        \"SN3_roads_train_AOI_3_Paris_geojson_roads_speed.tar.gz\",\n",
    "        \"SN3_roads_test_public_AOI_3_Paris.tar.gz\"\n",
    "    ]\n",
    "\n",
    "    # --- 1. Download Files ---\n",
    "    for s3_path in s3_files_to_download:\n",
    "        filename = os.path.basename(s3_path)\n",
    "        local_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "        else:\n",
    "            command = [\"aws\", \"s3\", \"cp\", s3_path, local_path, \"--no-sign-request\"]\n",
    "            run_command(command)\n",
    "\n",
    "    # --- 2. Extract Files ---\n",
    "    # The final directory that will be created\n",
    "    final_data_dir = os.path.join(output_dir, \"AOI_3_Paris\")\n",
    "    \n",
    "    # Check if the data has already been extracted\n",
    "    if os.path.exists(final_data_dir):\n",
    "         print(f\"Directory '{final_data_dir}' already exists. Assuming data is extracted. Skipping extraction.\")\n",
    "    else:\n",
    "        for filename in tar_files_to_extract:\n",
    "            local_path = os.path.join(output_dir, filename)\n",
    "            command = [\"tar\", \"-xzvf\", local_path]\n",
    "            # We run the command from within the output directory\n",
    "            run_command(command, working_dir=output_dir)\n",
    "\n",
    "    print(\"\\nData preparation complete!\")\n",
    "    print(f\"Your data should be located in: '{final_data_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09569e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61ef76",
   "metadata": {},
   "source": [
    "Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d24d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_stretch(band):\n",
    "    p2, p98 = np.percentile(band, (2, 98))\n",
    "    stretched = np.clip((band - p2) / (p98 - p2), 0, 1)\n",
    "    return stretched\n",
    "\n",
    "# PS-RGB file path\n",
    "file_path = 'data/AOI_3_Paris/PS-RGB/SN3_roads_train_AOI_3_Paris_PS-RGB_img111.tif'\n",
    "\n",
    "# Get the metadata\n",
    "with rasterio.open(file_path) as src:\n",
    "    image = src.read()\n",
    "    sat_meta = src.meta.copy()\n",
    "    width = src.width\n",
    "    height = src.height\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "\n",
    "\n",
    "# Apply stretching to each band\n",
    "stretched_image = np.stack([contrast_stretch(image[i]) for i in range(3)])\n",
    "\n",
    "# Check the number of bands\n",
    "print(f\"Number of bands: {image.shape[0]}\")\n",
    "\n",
    "# Plot the stretched image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(stretched_image.transpose(1, 2, 0))\n",
    "plt.title(\"Contrast-Stretched TIF Image (RGB Composite)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multispectral image (MS) and read the NIR band\n",
    "image_path = \"data/AOI_3_Paris/PS-MS/SN3_roads_train_AOI_3_Paris_PS-MS_img111.tif\"\n",
    "with rasterio.open(image_path) as src:\n",
    "    # Read the NIR band\n",
    "    nir_band = src.read(4)\n",
    "\n",
    "# Apply contrast stretching to the NIR band\n",
    "stretched_nir = contrast_stretch(nir_band)\n",
    "\n",
    "# Plot the NIR band as a grayscale image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(stretched_nir, cmap=\"gray\")\n",
    "plt.title(\"NIR Band (Contrast-Stretched)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faef2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON file\n",
    "gdf = gpd.read_file(\"data/AOI_3_Paris/geojson_roads_speed/SN3_roads_train_AOI_3_Paris_geojson_roads_speed_img118.geojson\")\n",
    "\n",
    "# Plotting the geometries\n",
    "gdf.plot(figsize=(5, 5), edgecolor='black', alpha=0.5)\n",
    "plt.title(\"GeoJSON Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Check the CRS of the image and GeoDataFrame\n",
    "print(f\"Image CRS: {src.crs}\")\n",
    "print(f\"GeoDataFrame CRS: {gdf.crs}\")\n",
    "\n",
    "# Align the CRS if necessary\n",
    "if gdf.crs != src.crs:\n",
    "    gdf = gdf.to_crs(src.crs)\n",
    "    print(f\"GeoDataFrame CRS after alignment: {gdf.crs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4382bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the satellite image\n",
    "image_path = \"data/AOI_3_Paris/PS-RGB/SN3_roads_train_AOI_3_Paris_PS-RGB_img341.tif\"\n",
    "\n",
    "with rasterio.open(image_path) as source:\n",
    "    image = source.read([1, 2, 3])\n",
    "    sat_meta = source.meta.copy()\n",
    "    width = source.width\n",
    "    height = source.height\n",
    "    transform = source.transform\n",
    "    crs = source.crs\n",
    "\n",
    "# Apply stretching to each band\n",
    "stretched_image = np.stack([contrast_stretch(image[i]) for i in range(3)])\n",
    "\n",
    "# Load the rasterized mask\n",
    "gdf = gpd.read_file(\"data/AOI_3_Paris/geojson_roads_speed/SN3_roads_train_AOI_3_Paris_geojson_roads_speed_img341.geojson\")\n",
    "\n",
    "# Prepare geometries and values (1 for roads, 0 elsewhere)\n",
    "\n",
    "projected_crs = \"EPSG:32631\"\n",
    "gdf_proj = gdf.to_crs(projected_crs)\n",
    "\n",
    "# Buffer in meters (e.g., 2 meters)\n",
    "buffered = gdf_proj.geometry.buffer(2)\n",
    "\n",
    "# Reproject back to raster CRS (EPSG:4326)\n",
    "buffered_wgs84 = buffered.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Prepare geometries for rasterization\n",
    "geoms = [(geom, 1) for geom in buffered_wgs84 if geom is not None and not geom.is_empty]\n",
    "\n",
    "# Rasterize\n",
    "mask = rasterize(\n",
    "    geoms,\n",
    "    out_shape=(height, width),\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    dtype='uint8'\n",
    ")\n",
    "\n",
    "# Plot the satellite image and mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Stretched satellite image\n",
    "axes[0].imshow(stretched_image.transpose(1, 2, 0))\n",
    "axes[0].set_title(f\"Contrast-Stretched TIF Image (RGB Composite) \\n Shape{stretched_image.shape}\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Road mask\n",
    "axes[1].imshow(mask, cmap=\"gray\")\n",
    "axes[1].set_title(f\"Road Mask \\n Shape{mask.shape}\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1debb",
   "metadata": {},
   "source": [
    "Dataset and the pytorchlightning DataModule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8eb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading and preprocessing satellite images and their\n",
    "    corresponding road network masks from GeoJSON files.\n",
    "\n",
    "    This class handles:\n",
    "    - Pairing of image (.tif) and mask (.geojson) files based on filenames.\n",
    "    - Robustly processing geospatial data to ensure perfect pixel alignment\n",
    "      between images and rasterized masks, regardless of their original\n",
    "      Coordinate Reference Systems (CRS).\n",
    "    - Applying data augmentations using the Albumentations library.\n",
    "    - Padding images and masks to be compatible with model architectures that\n",
    "      require specific input dimensions (e.g., divisible by 16 or 32).\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, augmentations=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory containing image files (.tif).\n",
    "            mask_dir (str): Path to the directory containing mask files (.geojson).\n",
    "            augmentations (albumentations.Compose, optional): An Albumentations\n",
    "                pipeline to apply to the image and mask. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        if not os.path.isdir(image_dir): raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "        if not os.path.isdir(mask_dir): raise FileNotFoundError(f\"Mask directory not found: {mask_dir}\")\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith((\".tif\", \".tiff\"))])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith(\".geojson\")])\n",
    "\n",
    "        self.index_to_mask = {self.extract_index(f): f for f in self.mask_files}\n",
    "\n",
    "        self.paired_files = [\n",
    "            (img, self.index_to_mask.get(self.extract_index(img)))\n",
    "            for img in self.image_files\n",
    "            if self.extract_index(img) in self.index_to_mask\n",
    "        ]\n",
    "        self.paired_files = [(img, mask) for img, mask in self.paired_files if mask is not None]\n",
    "        if not self.paired_files:\n",
    "            print(f\"CRITICAL WARNING: No image-mask pairs found after attempting to match filenames in {image_dir}.\")\n",
    "\n",
    "    def extract_index(self, filename):\n",
    "        \"\"\"Extracts the numerical index from a filename (e.g., 'img123').\"\"\"\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        match = re.search(r'img(\\d+)', base_name)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Loads a satellite image, performs contrast stretching, and returns it as a\n",
    "        NumPy array along with its geographic metadata.\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): The path to the image file.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - np.ndarray: The processed image as a uint8 NumPy array (H, W, C).\n",
    "                - dict: A dictionary of geographic information (crs, transform, etc.).\n",
    "        \"\"\"\n",
    "        with rasterio.open(image_path) as src:\n",
    "            geo_info = {\"crs\": src.crs, \"transform\": src.transform, \"height\": src.height, \"width\": src.width}\n",
    "            image_data = src.read(list(range(1, min(src.count, 3) + 1))).astype(np.float32)\n",
    "            if image_data.shape[0] < 3:\n",
    "                padding = np.zeros((3 - image_data.shape[0], geo_info[\"height\"], geo_info[\"width\"]), dtype=np.float32)\n",
    "                image_data = np.concatenate([image_data, padding], axis=0)\n",
    "\n",
    "        stretched_bands = []\n",
    "        for i in range(image_data.shape[0]):\n",
    "             min_val, max_val = np.percentile(image_data[i], (2, 98))\n",
    "             if np.isclose(max_val, min_val): stretched = np.zeros_like(image_data[i])\n",
    "             else: stretched = (image_data[i] - min_val) / (max_val - min_val)\n",
    "             stretched_bands.append(np.clip(stretched, 0, 1))\n",
    "        stretched_image_np = np.stack(stretched_bands)\n",
    "        t_img = stretched_image_np.transpose(1, 2, 0)\n",
    "        img_uint8 = (t_img * 255).astype(np.uint8)\n",
    "        return img_uint8, geo_info\n",
    "\n",
    "    def rasterize_mask(self, mask_path, image_geo_info):\n",
    "        \"\"\"\n",
    "        Creates a binary raster mask from a GeoJSON file, ensuring it is\n",
    "        perfectly aligned with its corresponding source image. Uses a robust\n",
    "        \"Project-Buffer-Reproject\" workflow.\n",
    "\n",
    "        Args:\n",
    "            mask_path (str): The path to the GeoJSON mask file.\n",
    "            image_geo_info (dict): The geographic metadata from the source image.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The rasterized binary mask as a uint8 NumPy array (H, W).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            gdf = gpd.read_file(mask_path)\n",
    "        except Exception:\n",
    "            return np.zeros((image_geo_info[\"height\"], image_geo_info[\"width\"]), dtype=np.uint8)\n",
    "\n",
    "        if gdf.crs and gdf.crs != image_geo_info[\"crs\"]:\n",
    "            try: gdf = gdf.to_crs(image_geo_info[\"crs\"])\n",
    "            except Exception: return np.zeros((image_geo_info[\"height\"], image_geo_info[\"width\"]), dtype=np.uint8)\n",
    "\n",
    "        if gdf.crs.is_geographic:\n",
    "            try: gdf = gdf.to_crs(\"EPSG:32631\")\n",
    "            except Exception: return np.zeros((image_geo_info[\"height\"], image_geo_info[\"width\"]), dtype=np.uint8)\n",
    "\n",
    "        valid_gdf = gdf[gdf.geometry.is_valid & ~gdf.geometry.is_empty]\n",
    "        if valid_gdf.empty: return np.zeros((image_geo_info[\"height\"], image_geo_info[\"width\"]), dtype=np.uint8)\n",
    "\n",
    "        # Using the buffer size that previously gave good results\n",
    "        buffer_distance_meters = 3.0\n",
    "        buffered_geometries = valid_gdf.geometry.buffer(buffer_distance_meters)\n",
    "        buffered_gdf = gpd.GeoDataFrame(geometry=buffered_geometries, crs=gdf.crs)\n",
    "        gdf_reprojected_for_raster = buffered_gdf.to_crs(image_geo_info[\"crs\"])\n",
    "\n",
    "        mask_array = rasterize(\n",
    "            gdf_reprojected_for_raster.geometry,\n",
    "            out_shape=(image_geo_info[\"height\"], image_geo_info[\"width\"]),\n",
    "            transform=image_geo_info[\"transform\"], fill=0, dtype='uint8', all_touched=True\n",
    "        )\n",
    "        if mask_array.ndim == 3: mask_array = mask_array.squeeze(0)\n",
    "        return mask_array\n",
    "\n",
    "    def __len__(self): return len(self.paired_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, mask_name = self.paired_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image_np, geo_info = self.process_image(image_path)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        mask_np = self.rasterize_mask(mask_path, geo_info)\n",
    "\n",
    "        pad_transform = A.PadIfNeeded(min_height=1312, min_width=1312, border_mode=cv2.BORDER_CONSTANT)\n",
    "        padded = pad_transform(image=image_np, mask=mask_np)\n",
    "        image_np_padded, mask_np_padded = padded['image'], padded['mask']\n",
    "\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image_np_padded, mask=mask_np_padded)\n",
    "            image_tensor, mask_tensor = augmented['image'], augmented['mask']\n",
    "        else:\n",
    "            image_tensor = torch.from_numpy(image_np_padded.transpose(2,0,1)).float()\n",
    "            mask_tensor = torch.from_numpy(mask_np_padded).unsqueeze(0).float()\n",
    "\n",
    "        return image_tensor.float(), mask_tensor.float()\n",
    "\n",
    "class GeoImageDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning DataModule for the road segmentation task.\n",
    "    Handles the creation of training, validation, and test dataloaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, augmentations_train=None, augmentations_val=None, batch_size=4, num_workers=0, split_perc=0.8, **kwargs):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.augmentations_train = augmentations_train\n",
    "        self.augmentations_val = augmentations_val\n",
    "        self.split_perc = split_perc\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        temp_dataset = GeoImageDataset(self.image_dir, self.mask_dir)\n",
    "        num_paired_files = len(temp_dataset)\n",
    "        train_size = int(self.split_perc * num_paired_files)\n",
    "        val_size = num_paired_files - train_size\n",
    "\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        indices = torch.randperm(num_paired_files, generator=g).tolist()\n",
    "        train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "        train_dataset_full = GeoImageDataset(self.image_dir, self.mask_dir, augmentations=self.augmentations_train)\n",
    "        self.train_dataset = torch.utils.data.Subset(train_dataset_full, train_indices)\n",
    "\n",
    "        val_dataset_full = GeoImageDataset(self.image_dir, self.mask_dir, augmentations=self.augmentations_val)\n",
    "        self.val_dataset = torch.utils.data.Subset(val_dataset_full, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, persistent_workers=True if self.num_workers > 0 else False, pin_memory=True, drop_last=True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, persistent_workers=True if self.num_workers > 0 else False, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef0a2e",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976732d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoImageSegmentModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning module for road segmentation from satellite images.\n",
    "\n",
    "    This class encapsulates the model architecture (U-Net), the loss function (a\n",
    "    combination of Dice and BCE), the optimization logic (AdamW with a cosine\n",
    "    annealing scheduler), and the training/validation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-4, weight_decay=1e-4, encoder_name=\"resnet50\"):\n",
    "        \"\"\"\n",
    "        Initializes the model, loss functions, and metrics.\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): The learning rate for the optimizer. Defaults to 1e-4.\n",
    "            weight_decay (float, optional): The weight decay for the AdamW optimizer. Defaults to 1e-4.\n",
    "            encoder_name (str, optional): The name of the encoder backbone to use from\n",
    "                segmentation-models-pytorch. Defaults to \"resnet50\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save hyperparameters to the checkpoint, allowing for easy reloading\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize the U-Net model from the segmentation-models-pytorch library\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights='imagenet', # Use pre-trained ImageNet weights for transfer learning\n",
    "            in_channels=3,\n",
    "            classes=1 # Binary output: road or not road\n",
    "        )\n",
    "\n",
    "        # Define the two components of the combination loss function\n",
    "        self.dice_loss = DiceLoss(mode='binary', from_logits=True)\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Initialize the IoU metric from torchmetrics for validation\n",
    "        self.iou_metric = BinaryJaccardIndex()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input batch of images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw output logits from the model.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx, stage):\n",
    "        \"\"\"\n",
    "        A common function for the training and validation steps to avoid code duplication.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing the images and masks.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            stage (str): The current stage, either \"train\" or \"val\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated loss for the batch.\n",
    "        \"\"\"\n",
    "        images, masks = batch\n",
    "        if masks.ndim == 3: masks = masks.unsqueeze(1)\n",
    "\n",
    "        # Get raw model outputs (logits)\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # Calculate the combination loss, giving more weight to Dice loss\n",
    "        # to better handle class imbalance.\n",
    "        loss = 0.8 * self.dice_loss(outputs, masks) + 0.2 * self.bce_loss(outputs, masks.float())\n",
    "\n",
    "        # Convert logits to probabilities and then to a binary prediction mask\n",
    "        preds_prob = torch.sigmoid(outputs)\n",
    "        preds_binary = (preds_prob > 0.5)\n",
    "        \n",
    "        # Calculate the IoU metric\n",
    "        iou = self.iou_metric(preds_binary, masks.int())\n",
    "\n",
    "        # Log the loss and IoU for monitoring in TensorBoard or other loggers\n",
    "        self.log(f'{stage}_loss', loss, on_epoch=True, prog_bar=True, logger=True, batch_size=images.size(0))\n",
    "        self.log(f'{stage}_iou', iou, on_epoch=True, prog_bar=True, logger=True, batch_size=images.size(0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Performs a single training step.\"\"\"\n",
    "        return self._common_step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Performs a single validation step.\"\"\"\n",
    "        return self._common_step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer (AdamW) and learning rate scheduler (CosineAnnealingLR).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the optimizer and the LR scheduler configuration.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=self.trainer.max_epochs, \n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\", # Step the scheduler at the end of each epoch\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cea4ac",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    image_dir,\n",
    "    mask_dir,\n",
    "    checkpoint_dir,\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-3,\n",
    "    encoder_name=\"resnet34\",\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    precision=\"16-mixed\",\n",
    "    augs_train=None,\n",
    "    augs_val=None,\n",
    "    resume_from_checkpoint=None\n",
    "):\n",
    "    \"\"\"\n",
    "    A complete training function that encapsulates the entire training workflow.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the directory with training/validation images.\n",
    "        mask_dir (str): Path to the directory with training/validation masks.\n",
    "        checkpoint_dir (str): Path to the directory where checkpoints will be saved.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 4.\n",
    "        num_workers (int, optional): Number of workers for the dataloader. Defaults to 2.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-4.\n",
    "        weight_decay (float, optional): Weight decay for the optimizer. Defaults to 1e-3.\n",
    "        encoder_name (str, optional): Encoder to use for the model. Defaults to \"resnet34\".\n",
    "        max_epochs (int, optional): Maximum number of epochs to train for. Defaults to 100.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        precision (int or str, optional): Training precision (e.g., 16 or '16-mixed'). Defaults to 16.\n",
    "        augs_train (A.Compose, optional): Custom training augmentations. If None, uses a strong default.\n",
    "        augs_val (A.Compose, optional): Custom validation augmentations. If None, uses a default.\n",
    "        resume_from_checkpoint (str, optional): Path to a checkpoint to resume training from. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Define default augmentations if none are provided\n",
    "    if augs_train is None:\n",
    "        augs_train = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Affine(\n",
    "                scale=(0.9, 1.1),\n",
    "                translate_percent=0.05,\n",
    "                rotate=(-15, 15),\n",
    "                p=0.5,\n",
    "                border_mode=cv2.BORDER_CONSTANT\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.1),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    if augs_val is None:\n",
    "        augs_val = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    # Initialize DataModule\n",
    "    data_module = GeoImageDataModule(\n",
    "        image_dir=image_dir,\n",
    "        mask_dir=mask_dir,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        augmentations_train=augs_train,\n",
    "        augmentations_val=augs_val\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    model = GeoImageSegmentModel(lr=lr, weight_decay=weight_decay, encoder_name=encoder_name)\n",
    "\n",
    "    # Initialize Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=f\"{encoder_name}-best-model-{{epoch:02d}}-{{val_iou:.4f}}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_iou\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_iou\", patience=patience, mode=\"max\")\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        precision=precision,\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    # Start Training\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        datamodule=data_module,\n",
    "        ckpt_path=resume_from_checkpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9020fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"data/AOI_3_Paris/PS-RGB\"\n",
    "MASK_DIR = \"data/AOI_3_Paris/geojson_roads\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "train_model(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    mask_dir=MASK_DIR,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba601b",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    use_close,\n",
    "    close_kernel_size,\n",
    "    use_min_object_size,\n",
    "    min_object_size,\n",
    "    image_dir,\n",
    "    mask_dir,\n",
    "    checkpoint_path,\n",
    "    max_batches=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a trained model from a checkpoint, runs it over a validation/test set,\n",
    "    and computes the IoU for both the raw and post-processed predictions.\n",
    "\n",
    "    Args:\n",
    "        use_close (bool): If True, applies morphological closing.\n",
    "        close_kernel_size (int): The size of the kernel for morphological closing.\n",
    "        use_min_object_size (bool): If True, removes small objects from the thick mask.\n",
    "        min_object_size (int): The minimum pixel area for an object to be kept.\n",
    "        image_dir (str): Path to the directory of evaluation images.\n",
    "        mask_dir (str): Path to the directory of evaluation masks.\n",
    "        checkpoint_path (str): Path to the .ckpt file of the trained model.\n",
    "        batch_size (int, optional): Batch size for evaluation. Defaults to 8.\n",
    "        num_workers (int, optional): Number of dataloader workers. Defaults to 2.\n",
    "        max_batches (int): limits the number of batches to evaluate. \n",
    "    \"\"\"\n",
    "    # --- 1. Load Model ---\n",
    "    model = GeoImageSegmentModel.load_from_checkpoint(checkpoint_path=CHECKPOINT_PATH)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded and moved to {device}\")\n",
    "\n",
    "    # --- 2. Prepare Data ---\n",
    "    augs_train = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.Affine(\n",
    "        scale=(0.9, 1.1),\n",
    "        translate_percent=0.05,\n",
    "        rotate=(-15, 15),\n",
    "        p=0.5,\n",
    "        border_mode=cv2.BORDER_CONSTANT\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.1),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    augs_val = A.Compose([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    data_module = GeoImageDataModule(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    augmentations_train=augs_train,\n",
    "    augmentations_val=augs_val\n",
    "    )\n",
    "    data_module.setup()\n",
    "    val_dataloader = data_module.val_dataloader()\n",
    "\n",
    "    # --- 3. Run Evaluation Loop ---\n",
    "    raw_iou_metric = BinaryJaccardIndex().to(device)\n",
    "    post_processed_iou_metric = BinaryJaccardIndex().to(device)\n",
    "\n",
    "    print(\"\\nStarting evaluation over the entire validation set...\")\n",
    "    for batch_idx, batch in enumerate(tqdm(val_dataloader, desc=\"Evaluating\")):\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        images, ground_truth_masks = batch\n",
    "        images = images.to(device)\n",
    "        ground_truth_masks = ground_truth_masks.unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            raw_predictions = (probabilities > 0.5)\n",
    "\n",
    "        # Update the raw IoU metric\n",
    "        raw_iou_metric.update(raw_predictions, ground_truth_masks.int())\n",
    "\n",
    "        # Post-process the predictions and update the second metric\n",
    "        post_processed_preds_batch = []\n",
    "        for i in range(images.shape[0]):\n",
    "            raw_pred_np = raw_predictions[i].squeeze().cpu().numpy()\n",
    "            post_processed_pred = post_process_mask(\n",
    "                raw_pred_np,\n",
    "                use_close=use_close,\n",
    "                close_kernel_size=close_kernel_size,\n",
    "                use_min_object_size=use_min_object_size,\n",
    "                min_object_size=min_object_size,\n",
    "            )\n",
    "            post_processed_preds_batch.append(post_processed_pred)\n",
    "\n",
    "        # Convert the list of post-processed numpy arrays to a tensor for the metric\n",
    "        pp_preds_tensor = torch.from_numpy(np.stack(post_processed_preds_batch)).unsqueeze(1).to(device)\n",
    "        post_processed_iou_metric.update(pp_preds_tensor, ground_truth_masks.int())\n",
    "\n",
    "    # --- 4. Print Final Results ---\n",
    "    # Compute the final macro-IoU score from the accumulated stats\n",
    "    avg_raw_iou = raw_iou_metric.compute()\n",
    "    avg_pp_iou = post_processed_iou_metric.compute()\n",
    "\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "    print(f\"Average RAW Prediction IoU:      {avg_raw_iou.item():.4f}\")\n",
    "    print(f\"Average POST-PROCESSED IoU:      {avg_pp_iou.item():.4f}\")\n",
    "\n",
    "    return avg_pp_iou\n",
    "\n",
    "def plot_evaluation_results(\n",
    "    CHECKPOINT_PATH,\n",
    "    image_dir,\n",
    "    mask_dir,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a trained model from a checkpoint, runs it over a batch from the validation set,\n",
    "    and visualizes the original images, ground truth masks, raw predictions, and post-processed predictions.\n",
    "\n",
    "    Args:\n",
    "        CHECKPOINT_PATH (str): Path to the .ckpt file of the trained model.\n",
    "        image_dir (str): Path to the directory of evaluation images.\n",
    "        mask_dir (str): Path to the directory of evaluation masks.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Your Trained Model ---\n",
    "    model = GeoImageSegmentModel.load_from_checkpoint(checkpoint_path=CHECKPOINT_PATH)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded and moved to {device}\")\n",
    "\n",
    "\n",
    "    # --- 2. Set up the DataModule to get a batch ---\n",
    "    augs_train = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5), \n",
    "        A.VerticalFlip(p=0.5),  \n",
    "        A.RandomRotate90(p=0.3),  \n",
    "        A.Affine(\n",
    "            scale=(0.9, 1.1),      \n",
    "            translate_percent=0.05,\n",
    "            rotate=(-15, 15),      \n",
    "            p=0.5,\n",
    "            border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(  \n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.3\n",
    "        ),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.1),  \n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),   \n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    augs_val = A.Compose([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    data_module = GeoImageDataModule(\n",
    "        image_dir=image_dir,\n",
    "        mask_dir=mask_dir,\n",
    "        batch_size=4,\n",
    "        num_workers=2,\n",
    "        augmentations_train=augs_train,\n",
    "        augmentations_val=augs_val\n",
    "    )\n",
    "    data_module.setup()\n",
    "\n",
    "    # Create the dataloader\n",
    "    val_dataloader = data_module.val_dataloader()\n",
    "\n",
    "    # Create the iterator object ONCE\n",
    "    data_iterator = iter(val_dataloader)\n",
    "\n",
    "    # Get the FIRST batch\n",
    "    images, ground_truth_masks = next(data_iterator)\n",
    "    print(\"Fetched batch 1\")\n",
    "\n",
    "    # --- 3. Get Model Predictions for the Batch ---\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(images)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        predictions = (probabilities > 0.5).cpu()\n",
    "\n",
    "\n",
    "    # --- 4. Visualize Results for Each Image in the Batch ---\n",
    "\n",
    "    batch_size = images.shape[0]\n",
    "    plt.figure(figsize=(20, 5 * batch_size)) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get the i-th image, ground truth, and prediction from the batch\n",
    "\n",
    "        image_to_plot = images[i]\n",
    "        gt_mask_to_plot = ground_truth_masks[i].squeeze()\n",
    "        pred_mask_to_plot = predictions[i].squeeze()\n",
    "\n",
    "        # --- Apply the full post-processing pipeline ---\n",
    "        final_prediction = post_process_mask(pred_mask_to_plot.numpy())\n",
    "\n",
    "        # --- Plotting ---\n",
    "        # Column 1: Original Image\n",
    "        plt.subplot(batch_size, 4, i * 4 + 1)\n",
    "        plt.imshow(visualize_image(image_to_plot))\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Column 2: Ground Truth\n",
    "        plt.subplot(batch_size, 4, i * 4 + 2)\n",
    "        plt.imshow(gt_mask_to_plot, cmap='gray')\n",
    "        plt.title(f\"Ground Truth {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Column 3: Raw Model Prediction (Thick)\n",
    "        plt.subplot(batch_size, 4, i * 4 + 3)\n",
    "        plt.imshow(pred_mask_to_plot, cmap='gray')\n",
    "        plt.title(f\"Raw Prediction {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Column 4: Fully Post-Processed Prediction\n",
    "        plt.subplot(batch_size, 4, i * 4 + 4)\n",
    "        plt.imshow(final_prediction, cmap='gray')\n",
    "        plt.title(f\"Post-Processed {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your data and best model\n",
    "CHECKPOINT_PATH = \"checkpoints/baseline-best-model-epoch=76-val_iou=0.5967.ckpt\"\n",
    "IMAGE_DIR = \"data/AOI_3_Paris/PS-RGB\"\n",
    "MASK_DIR = \"data/AOI_3_Paris/geojson_roads\"\n",
    "BATCH_SIZE = 6 # Use a larger batch size for evaluating more of the dataset (the val dataset will have 13 batches in total)\n",
    "\n",
    "# Define the post-processing parameters you want to test\n",
    "# These could be the best ones you found from your Optuna study\n",
    "best_params = {\n",
    "    'use_close': True,\n",
    "    'close_kernel_size': 5,\n",
    "    'use_min_object_size': True,\n",
    "    'min_object_size': 1500,\n",
    "}\n",
    "\n",
    "# Run the evaluation\n",
    "run_evaluation(use_close=True,\n",
    "                    close_kernel_size=5\n",
    "                    ,use_min_object_size=True,\n",
    "                    min_object_size=1500,\n",
    "                    max_batches=BATCH_SIZE,\n",
    "                    image_dir=IMAGE_DIR,\n",
    "                    mask_dir=MASK_DIR,\n",
    "                    checkpoint_path=CHECKPOINT_PATH)\n",
    "\n",
    "# Visualize the results (the first batch)\n",
    "plot_evaluation_results(CHECKPOINT_PATH=CHECKPOINT_PATH,\n",
    "                        image_dir=IMAGE_DIR,\n",
    "                        mask_dir=MASK_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
